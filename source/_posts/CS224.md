---
title: CS224
date: 2018-03-05 22:50:40
tags: [NLP,ML]
---

##### Word2Vec 词向量
1. one-hot p=[0,.,0,1,0,..0]用 Hash 表给每个词分配一个编号
- 维数灾难;词汇鸿沟”现象：任意两个词之间都是孤立的。
2. deeplearning：Distributed Representation实数向量[0.792, −0.177, −0.107, 0.109, −0.542, …]维度以 50 维和 100 维比较常见。相似的词可以计算距离or Cos
> ***Distributed representation 用来表示词，通常被称为“Word Representation”或“Word Embedding”，中文俗称“词向量”。***
相似词的词向量距离相近，这就让基于词向量设计的一些模型自带平滑功能，让模型看起来非常的漂亮。
> σ() softmax number -> 分布
- ??证明σ(-x) = 1-σ(x)

###### skip gram
一个词有两个向量，v 中心词向量 u 上下文向量
U(like)’V(deep) 两个词共同出现的概率（max)
分母是词汇表

- binary logistic regression

- J(θ)：T窗口;j~P(w)随机抽取语料库的单词（min)
	- 随机 unigram distribution U(w) 3/4

- Stochastic gradients
- continuous bag of words(CBOW)：通过周围词向量的和来预测中心词而不是单个邻接词（skip gram)
- Cross entropy 交叉熵（loss for softmax)

Context({})向模板提供数据

- 对co-occurrence矩阵降维： ！！！SVD singular value decomposition

##### 奇异值分解$A=UΣV^T$
- m*n的矩阵对角化->特征值、行列式、幂、指数函数
	- n阶矩阵相似于对角阵 <=>有n个线性无关的特征向量
		- 实对称**正交**相似于对角阵
		- $Q^TAQ=Λ=diag(λ1...λn)$（特征值）
		- 正交矩阵$Q=(v_1...v_n)$：$Av_i=λ_iv_i$ $Q^TQ=E$
		- $AV = UΣ$->V和U展开->Σ展开->r=rank(A)->$Av_i=σ_iu_i(i∈（1,r));Av_j=0$->两边转置->$A^TU=VΣ^T$->$A^Tu_i=σ_iv_i$
		- $A^TAv_i = σ_i^2v_i$(特，特向)
		- $AA^Tu_i = σ_i^2u_i$
###### 原理
- A(mxn)->$AA^T$为m阶实对称$A^TA$n阶实对称
	1. 特征值非负 证明：$x^TA^TAx = λx^Tx$->$||Ax||^2(>=0)=λ||x||^2(>0)$
	2. $A^TA$与$AA^T$非零特征集合相同 证明:$r(AA^T)=r(A^T),r(A^TA)=r(A)$-> $r(AA^T)=r(A^TA)=r(A)=R$
###### 几何意义：旋转、伸缩、旋转
- $R^n->R^m x->Ax$变换
- $x-V^Tx$旋转
- $Σ$ 对前r个分量做伸缩


- ??交叉熵与Kullback-Leibler
- derivative导数，gradient梯度
- $timeit 
-  max-margin
- sigmoid函数求导:$ \frac{σ(y)}{dy}=σ(y)·(1-σ(y))$,接近0 梯度最大

#### 语言模型
##### Dynamic Memory Network
[Dynamic Memory Network](https://metamind.io/research/new-deep-learning-model-understands-and-answers-questions)
[A Joint Many-Task Model:Growing a Neural Network for Multiple NLP Tasks](https://metamind.io/research/multiple-different-natural-language-processing-tasks-in-a-single-deep-model
)

#### 统计学习方法
##### 学习策略
- 感知机：
1. $w·x+b=0$是平面
2. 误分点到平面距离$(w·x_i+b)$,乘错误点的输出$·y_i$ 正确的点 输出和平面函数是一样的。 
3. 损失函数：所有误分点距离取正
4. min损失函数，对w,b求导；梯度下降 w+学习率(0,1]·对w求导；同理b。

- 损失函数3种：
- 结构风险函数：
- 正则化

